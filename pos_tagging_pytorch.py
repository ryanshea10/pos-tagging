# -*- coding: utf-8 -*-
"""POS_Tagging.ipynb

Automatically generated by Colaboratory.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import copy

torch.manual_seed(1)

# write output (in proper format) for early stopping
def write_output(data):
  with torch.no_grad():
    with open("dev_output.txt", 'w') as inp:
      for line in data:
        inputs = prepare_sequence(line[0], word_to_ix)
        tag_scores = model(inputs)
        outputs = [int(np.argmax(score)) for score in tag_scores]
        formatted_output = ' '.join([f"{word}|||{ix_to_tag[tag_id]}" for word,tag_id in zip(line[0],outputs)])
        inp.write(formatted_output + '\n')

def convert_data(data):
	return [([t[0] for t in d],[t[1] for t in d]) for d in data]

# function to read part of speech data
def read_data(path):
	with open(path) as inp:
		lines = inp.readlines()
	data = []
	for line in lines:
		line = line.strip().split()
		sent = []
		for seq in line:
			seq = seq.split('|||')
			word, tag = seq
			sent.append((word,tag))
		data.append(sent)
	return data

# function to compute tagging accuracy
def get_accuracy(output, correct):
	count_correct = 0
	count_total = 0
    
	for outputs, correct_vals in zip(output,correct):
		check_correctness = [1 if out[1] == cor[1] else 0 for out,cor in zip(outputs,correct_vals)]
		count_correct += sum(check_correctness)
		count_total += len(check_correctness)
        
	return count_correct/count_total


# convert words to ids
def prepare_sequence(seq, to_ix):
  idxs=[to_ix[w] if w in to_ix else to_ix['<unk>'] for w in seq]
  return torch.tensor(idxs, dtype=torch.long)


# class to define lstm model
class LSTM(nn.Module):
    
    # define model parameters
	def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
		super(LSTM, self).__init__()
		self.hidden_dim = hidden_dim
		self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
		self.lstm = nn.LSTM(embedding_dim, hidden_dim)
		self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

	# define forward pass
	def forward(self, sentence):
		# get embeddings
		embeds = self.word_embeddings(sentence)
		# get lstm output
		lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
		tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
		# convert to a log probability distribution
		tag_scores = F.log_softmax(tag_space, dim=1)
		return tag_scores

TRAINING_FILE = "training_set.txt"
training_data_no_unk = convert_data(read_data(TRAINING_FILE))

sentences=[i[0] for i in training_data_no_unk]

# functions to deal with unknown words
def get_unks(sentences, n=1):
  count_dict={}
  
  for sentence in sentences:
    for word in sentence:
      count_dict[word]=count_dict.get(word, 0) + 1

  unk_dict={key:value for key, value in count_dict.items() if value<=n}
  return unk_dict

def insert_unks(sentences, unk_dict):
  new_sentences=[]
  
  for sentence in sentences:
    new_sent=[]
    for word in sentence:
      new_sent.append('<unk>' if (word in unk_dict.keys()) else word)
    new_sentences.append(new_sent)
    
  return new_sentences

def substitute_with_unk(training_data, n=1):
  sentences=[i[0] for i in training_data]
  pos=[i[1] for i in training_data]
  unks=get_unks(sentences)
  sent_with_unks=insert_unks(sentences, unks)
  
  return list(zip(sent_with_unks,pos))

training_data_unk=substitute_with_unk(training_data_no_unk)

training_data=training_data_unk

DEV_FILE="dev_set.txt"
dev_data=convert_data(read_data(DEV_FILE))


# Get token and tag vocabularies from the training set, and map them to integer IDs
word_to_ix = {}
ix_to_word = {}
tag_to_ix = {}
ix_to_tag = {}
for sent, tags in training_data:
	for word in sent:
		if word not in word_to_ix:
			word_to_ix[word] = len(word_to_ix)
			ix_to_word[word_to_ix[word]] = word
	for tag in tags:
		if tag not in tag_to_ix:
			tag_to_ix[tag] = len(tag_to_ix)
			ix_to_tag[tag_to_ix[tag]] = tag

# Hyperparameters
EMBEDDING_DIM = 32
HIDDEN_DIM = 32
count=0
patience=6
best_acc=-1
current_acc=-1

# Initialize the model
model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training loop
# 50 epochs is too much, run until early stopping kicks in
for epoch in range(50):
  print("Starting epoch {epoch}...")
  for sentence, tags in training_data:
    model.zero_grad()

    # prepare inputs
    sentence_in = prepare_sequence(sentence, word_to_ix)
    targets = prepare_sequence(tags, tag_to_ix)

    #forward pass
    tag_scores = model(sentence_in)

    # update parameters
    loss = loss_function(tag_scores, targets)
    loss.backward()
    optimizer.step()

  # early stopping
  write_output(dev_data)
  current_acc=get_accuracy(read_data("dev_output.txt"), read_data('dev_set.txt'))
  print(current_acc)
  if current_acc>best_acc:
    best_model=copy.deepcopy(model)
    best_acc=current_acc
    count=0
  else:
    count+=1
  if count>=patience:
    break

#save trained model
torch.save(model, 'nlp_model.pt')

#load saved model
saved_model = torch.load('nlp_model.pt')
saved_model.eval()

TEST_FILE = "test_set.txt"
test_data = convert_data(read_data(TEST_FILE))

with torch.no_grad():
	with open("mymodel_output.txt", 'w') as inp:
		for line in test_data:
			inputs = prepare_sequence(line[0], word_to_ix)
			tag_scores = best_model(inputs)
			outputs = [int(np.argmax(score)) for score in tag_scores]
			formatted_output = ' '.join([f"{word}|||{ix_to_tag[tag_id]}" for word,tag_id in zip(line[0],outputs)])
			inp.write(formatted_output + '\n')

get_accuracy(read_data('mymodel_output.txt'),read_data('test_set.txt'))
